{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import myd2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data_nmt(path='E:/Datasets/Tatoeba-fra-eng/fra-eng/fra.txt'):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "    \n",
    "raw_text = read_data_nmt()\n",
    "print(raw_text[: 75])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go .\tva !\n",
      "hi .\tsalut !\n",
      "run !\tcours !\n",
      "run !\tcourez !\n",
      "who ?\tqui ?\n",
      "wow !\tça alors !\n"
     ]
    }
   ],
   "source": [
    "def preprocess_nmt(text):\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)]\n",
    "\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.'], ['hi', '.'], ['run', '!'], ['run', '!'], ['who', '?'], ['wow', '!']]\n",
      "[['va', '!'], ['salut', '!'], ['cours', '!'], ['courez', '!'], ['qui', '?'], ['ça', 'alors', '!']]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def tokenize_nmt(text, num_examples=None):\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    \n",
    "    return source, target\n",
    "\n",
    "source, target = tokenize_nmt(text)\n",
    "print(source[:6])\n",
    "print(target[:6])\n",
    "\n",
    "print('cuff' in set([token for lines in source for token in lines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10012\n",
      "0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "src_vocab = myd2l.Vocab(source, 2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "print(len(src_vocab))\n",
    "print(src_vocab['cuff'])\n",
    "print(src_vocab['<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    if(len(line) > num_steps):\n",
    "        line = line[: num_steps]\n",
    "    else:\n",
    "        line += [padding_token] * (num_steps - len(line))\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 4, 1, 1, 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncate_pad(src_vocab[source[0]], 5, src_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2944,    4,    3,    1,    1,    1,    1,    1])\n",
      "tensor([3, 3, 3,  ..., 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "def build_arr_nmt(lines, vocab, num_steps):\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(dim=1)\n",
    "\n",
    "    return array, valid_len\n",
    "\n",
    "array, valid_len = build_arr_nmt(source, src_vocab, 8)\n",
    "print(array[1])\n",
    "print(valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    src_vocab = myd2l.Vocab(source, 2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = myd2l.Vocab(target, 2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    src_array, src_valid_len = build_arr_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_arr_nmt (target, tgt_vocab, num_steps)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return data_iter, src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[30, 38,  4,  3,  1,  1,  1,  1],\n",
      "        [68, 25,  5,  3,  1,  1,  1,  1]], dtype=torch.int32)\n",
      "X's valid length:  tensor([4, 4])\n",
      "Y:  tensor([[ 0,  5,  3,  1,  1,  1,  1,  1],\n",
      "        [89,  5,  3,  1,  1,  1,  1,  1]], dtype=torch.int32)\n",
      "Y' valid length:  tensor([3, 3])\n"
     ]
    }
   ],
   "source": [
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(2, 8)\n",
    "for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
    "    print('X: ', X.type(torch.int32))\n",
    "    print('X\\'s valid length: ', X_valid_len)\n",
    "    print('Y: ', Y.type(torch.int32))\n",
    "    print('Y\\' valid length: ', Y_valid_len)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
